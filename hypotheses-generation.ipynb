{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Raleigh_City_Council> --> <wikicat_City_councils_in_the_United_States>\n",
      "<Raleigh_Aitchison> --> <wikicat_Oklahoma_City_Indians_players>\n",
      "<Sim_Raleigh> --> <wikicat_Hull_City_A.F.C._players>\n",
      "<Walter_Raleigh_(professor)> --> <wikicat_People_educated_at_the_City_of_London_School>\n",
      "<Raleigh_DeGeer_Amyx> --> <wikicat_People_from_Kansas_City,_Kansas>\n",
      "<Raleigh_Croshaw> --> <wikicat_People_from_Elizabeth_City_County,_Virginia>\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "# Connect\n",
    "myhost='localhost'\n",
    "myuser='myuser'\n",
    "mypasswd='mypass'\n",
    "mydb='mydb'\n",
    "connection = pymysql.connect(host=myhost,\n",
    "                             user=myuser,\n",
    "                             passwd=mypasswd,\n",
    "                             db=mydb)\n",
    "\n",
    "#print (\"connect successful!!\")\n",
    "\n",
    "text = \"A hacker gained access to the PHI for the covered entity's patients.\"\n",
    " \n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Execute SQL select statement\n",
    "        cursor.execute(\"SELECT instance, class FROM simple_types where instance like '%Raleigh%' and class like '%CITY%'\")\n",
    "        # Commit your changes if writing\n",
    "        # In this case, we are only reading data\n",
    "        # db.commit()\n",
    "        \n",
    "        # Get the number of rows in the resultset\n",
    "        numrows = cursor.rowcount\n",
    "        \n",
    "        # Get and display one row at a time\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            print(row[0], \"-->\", row[1])\n",
    "            \n",
    "# Close the connection\n",
    "finally:\n",
    "    # Close connection.\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ISIS members threw stones in Paris.\n",
      "Tokenize: ['ISIS', 'members', 'threw', 'stones', 'in', 'Paris', '.']\n",
      "Part of Speech: [('ISIS', 'NNP'), ('members', 'NNS'), ('threw', 'VBD'), ('stones', 'NNS'), ('in', 'IN'), ('Paris', 'NNP'), ('.', '.')]\n",
      "Named Entities: [('ISIS', 'ORGANIZATION'), ('members', 'O'), ('threw', 'O'), ('stones', 'O'), ('in', 'O'), ('Paris', 'CITY'), ('.', 'O')]\n",
      "Constituency Parsing: (ROOT\n",
      "  (S\n",
      "    (NP (NNP ISIS) (NNS members))\n",
      "    (VP (VBD threw)\n",
      "      (NP (NNS stones))\n",
      "      (PP (IN in)\n",
      "        (NP (NNP Paris))))\n",
      "    (. .)))\n",
      "Dependency Parsing: [('ROOT', 0, 3), ('compound', 2, 1), ('nsubj', 3, 2), ('dobj', 3, 4), ('case', 6, 5), ('nmod', 3, 6), ('punct', 3, 7)]\n"
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('C:/Users/Nirav/libraries/stanford-corenlp-full')\n",
    "connection = pymysql.connect(host=myhost,\n",
    "                             user=myuser,\n",
    "                             passwd=mypasswd,\n",
    "                             db=mydb)\n",
    "\n",
    "try: \n",
    "    with connection.cursor() as cursor:\n",
    "        #cursor.execute(\"Select text, day, place, attacker, target, instrument from cyberattacks\")\n",
    "        cursor.execute(\"Select 'ISIS members threw stones in Paris.'\")\n",
    "        numrows = cursor.rowcount\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            sentence = row[0]\n",
    "\n",
    "            print('Sentence:', sentence)\n",
    "            print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "            print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "            print('Named Entities:', nlp.ner(sentence))\n",
    "            print('Constituency Parsing:', nlp.parse(sentence))\n",
    "            print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "finally: \n",
    "    connection.close()\n",
    "    nlp.close() # Do not forget to close! The backend server will consume a lot memery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIS ORGANIZATION\n",
      "Paris CITY\n",
      "threw VBD\n"
     ]
    }
   ],
   "source": [
    "# importing StandfordCoreNLP to tokenize, tag, and ner\n",
    "nlp = StanfordCoreNLP('C:/Users/Nirav/libraries/stanford-corenlp-full')\n",
    "\n",
    "sentence = \"ISIS members threw stones in Paris.\"\n",
    "sentence_tokens = nlp.word_tokenize(sentence)\n",
    "sentence_tags = nlp.pos_tag(sentence)\n",
    "sentence_ner = nlp.ner(sentence)\n",
    "#sentence_parse = nlp.parse(sentence)\n",
    "#sentence_dependency = nlp.dependency_parse(sentence)\n",
    "\n",
    "to_replace_ners = []\n",
    "to_replace_verbs = []\n",
    "\n",
    "for (i, j) in sentence_ner:\n",
    "    #print(i, j)\n",
    "    if(j!='O'):\n",
    "        print(i, j)\n",
    "        to_replace_ners.append((i,j))\n",
    "        \n",
    "verb_check = 0\n",
    "        \n",
    "for (i, j) in sentence_tags:\n",
    "    if(verb_check == 1):\n",
    "        verb = verb + '_' + i\n",
    "        to_replace_verbs = [verb]\n",
    "        verb_check = 0\n",
    "        print(verb)\n",
    "    \n",
    "    if(j=='VBD'):\n",
    "        print(i, j)\n",
    "        verb_check = 1\n",
    "        verb = i\n",
    "    \n",
    "        \n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing word2vec to find similarity and neighboring words\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Nirav/libraries/GoogleNews-vectors-negative300.bin.gz', binary=True) \n",
    "\n",
    "topk = 10\n",
    "replacement_ners = []\n",
    "replacement_verbs = []\n",
    "\n",
    "for (i, j) in to_replace_ners:\n",
    "    similar_ners = model.most_similar(i, topk)\n",
    "    replacements_ners.append((i, similar_ners))\n",
    "\n",
    "print(replacement_ners)\n",
    "    \n",
    "for verb in replacements_verbs:\n",
    "    similar_verbs = model.most_similar(verb, topk)\n",
    "    replacement_verbs.append((verb,similar_verbs))\n",
    "\n",
    "print(replacement_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
