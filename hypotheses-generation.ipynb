{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing configuration\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.txt\")\n",
    "\n",
    "word2vec_path = config.get(\"configuration\",\"word2vec_path\")\n",
    "stanford_corenlp_path = config.get(\"configuration\",\"stanford_corenlp_path\")\n",
    "\n",
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "# Connect\n",
    "mydbhost = config.get(\"configuration\",\"mydbhost\")\n",
    "mydbuser = config.get(\"configuration\",\"mydbuser\")\n",
    "mydbpasswd = config.get(\"configuration\",\"mydbpasswd\")\n",
    "mydbdb = config.get(\"configuration\",\"mydbdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stanfordcorenlp import StanfordCoreNLP\n",
    "# nlp = StanfordCoreNLP(stanford_corenlp_path)\n",
    "# connection = pymysql.connect(host=mydbhost,\n",
    "#                              user=mydbuser,\n",
    "#                              passwd=mydbpasswd,\n",
    "#                              db=mydbdb)\n",
    "\n",
    "# try: \n",
    "#     with connection.cursor() as cursor:\n",
    "#         #cursor.execute(\"Select text, day, place, attacker, target, instrument from cyberattacks\")\n",
    "#         cursor.execute(\"Select 'ISIS members threw stones in Paris.'\")\n",
    "#         numrows = cursor.rowcount\n",
    "#         for x in range(0, numrows):\n",
    "#             row = cursor.fetchone()\n",
    "#             sentence = row[0]\n",
    "\n",
    "#             print('Sentence:', sentence)\n",
    "#             print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "#             print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "#             print('Named Entities:', nlp.ner(sentence))\n",
    "#             print('Constituency Parsing:', nlp.parse(sentence))\n",
    "#             print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "# finally: \n",
    "#     connection.close()\n",
    "#     nlp.close() # Do not forget to close! The backend server will consume a lot memery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "# Tree syntax of natural language: http://www.cs.cornell.edu/courses/cs474/2004fa/lec1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing word2vec to find similarity and neighboring words\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=500000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Russian hackers sent phishing emails to compromise NCSU's backup servers.\n",
      "Part of Speech: [('Russian', 'JJ'), ('hackers', 'NNS'), ('sent', 'VBN'), ('phishing', 'VBG'), ('emails', 'NNS'), ('to', 'TO'), ('compromise', 'VB'), ('NCSU', 'NNP'), (\"'s\", 'POS'), ('backup', 'NN'), ('servers', 'NNS'), ('.', '.')]\n",
      "Named Entities: [('Russian', 'NATIONALITY'), ('hackers', 'O'), ('sent', 'O'), ('phishing', 'O'), ('emails', 'O'), ('to', 'O'), ('compromise', 'O'), ('NCSU', 'ORGANIZATION'), (\"'s\", 'O'), ('backup', 'O'), ('servers', 'O'), ('.', 'O')]\n",
      "[('Russian', 'NATIONALITY'), ('NCSU', 'ORGANIZATION')]\n",
      "['sent']\n",
      "[('sent_phishing', 'phishing')]\n",
      "['Russian']\n"
     ]
    }
   ],
   "source": [
    "# importing StandfordCoreNLP to tokenize, tag, and ner\n",
    "nlp = StanfordCoreNLP(stanford_corenlp_path)\n",
    "\n",
    "#sentence = \"The hacker group Anonymous installed ransomware to steal cryptocurrency.\"\n",
    "#sentence = \"ISIL members launched DDOS attack on European Union.\"\n",
    "sentence = \"Russian hackers sent phishing emails to compromise NCSU's backup servers.\"\n",
    "\n",
    "sentence_tokens = nlp.word_tokenize(sentence)\n",
    "sentence_tags = nlp.pos_tag(sentence)\n",
    "sentence_ner = nlp.ner(sentence)\n",
    "#sentence_parse = nlp.parse(sentence)\n",
    "#sentence_dependency = nlp.dependency_parse(sentence)\n",
    "\n",
    "print('Sentence:', sentence)\n",
    "# print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "#print('Constituency Parsing:', nlp.parse(sentence))\n",
    "#print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "\n",
    "to_replace_ners = []\n",
    "to_replace_verbs = []\n",
    "to_replace_verbphrases = []\n",
    "to_replace_adjectives = []\n",
    "to_replace_adjphrase = []\n",
    "# prev_ner = \"\"\n",
    "# current_entity = \"\"\n",
    "# prev_entity_exists = 0\n",
    "\n",
    "for (i, j) in sentence_ner:\n",
    "#     print(i, j)\n",
    "    if(j!='O'):\n",
    "        to_replace_ners.append((i, j))\n",
    "#         print('PREV NER', prev_ner, 'Current NER', j)\n",
    "#         if(j==prev_ner): \n",
    "#             current_entity = current_entity + '_' + i\n",
    "#             print(current_entity)\n",
    "#             prev_ner = j\n",
    "#             prev_entity_exists = 1\n",
    "#         elif(prev_entity_exists): \n",
    "#             to_replace_ners.append((current_entity, prev_ner))\n",
    "#             prev_entity_exists = 0\n",
    "#             current_entity = i\n",
    "#             prev_ner = j\n",
    "#     elif(current_entity != \"\"):     \n",
    "#         to_replace_ners.append((current_entity, prev_ner))\n",
    "#         prev_ner = \"\"\n",
    "#         current_entity = \"\"\n",
    "    \n",
    "        \n",
    "verb_check = 0\n",
    "adj_check = 0\n",
    "        \n",
    "for (i, j) in sentence_tags:\n",
    "    if(verb_check == 1):\n",
    "        verbphrase = verb + '_' + i\n",
    "        to_replace_verbphrases.append((verbphrase, i))\n",
    "        verb_check = 0\n",
    "        #print(verbphrase)\n",
    "    \n",
    "    if(j=='VBD' or j=='VBZ' or j == 'VBP' or j == 'VBN'):\n",
    "        #print(i, j)\n",
    "        to_replace_verbs.append(i)\n",
    "        verb_check = 1\n",
    "        verb = i\n",
    "    \n",
    "    if(j == 'JJ'):\n",
    "        to_replace_adjectives.append(i)\n",
    "        adj = j\n",
    "        adj_check = 1\n",
    "\n",
    "# to_replace_entities = []\n",
    "# prev_type = \"\"\n",
    "# prev_entity = \"\"\n",
    "# cur_entity = \"\"\n",
    "# lastindex = -1\n",
    "# for (entity, type) in to_replace_ners:\n",
    "#     print(entity, type, prev_entity, prev_type, cur_entity)\n",
    "    \n",
    "#     if(type==prev_type):\n",
    "#         to_replace_entities[lastindex] = (to_replace_ners[lastindex][0] + '_' + entity, type)\n",
    "#     else:\n",
    "#         to_replace_entities.append((cur_entity, prev_type))\n",
    "#         cur_entity = entity\n",
    "#         lastindex = lastindex + 1\n",
    "    \n",
    "#     prev_type = type\n",
    "#     #prev_entity = entity\n",
    "\n",
    "print(to_replace_ners)\n",
    "# print(to_replace_entities)\n",
    "print(to_replace_verbs)\n",
    "print(to_replace_verbphrases)\n",
    "print(to_replace_adjectives)\n",
    "        \n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Russian', [('Ukrainian', 0.6549570560455322), ('Kazakh', 0.6192217469215393), ('Latvian', 0.613041341304779), ('Bulgarian', 0.6038623452186584), ('Belarussian', 0.6027601957321167), ('Russians', 0.6012547016143799), ('Ukranian', 0.5897480249404907), ('Romanian', 0.5878135561943054), ('Uzbek', 0.5867130160331726), ('Belarusian', 0.5811043977737427)]), ('Carolina', [('Charleston', 0.532617449760437), ('Alabama', 0.5290449261665344), ('Greenville', 0.5225397348403931), ('North_Carolina', 0.5223299860954285), ('county', 0.5205069780349731), ('Jacksonville', 0.5100679397583008), ('Lowcountry', 0.5022171139717102), ('Myrtle_Beach', 0.4985104203224182), ('downtown', 0.49651625752449036), ('town', 0.49576088786125183)])]\n",
      "[('sent', [('sending', 0.7625813484191895), ('send', 0.736850917339325), ('sends', 0.645559549331665), ('forwarded', 0.618112325668335), ('mailed', 0.5840898752212524), ('despatched', 0.5625948309898376), ('Sending', 0.5511707663536072), ('dispatched', 0.5449301600456238), ('e_mailed', 0.5339891910552979), ('circulated', 0.5030714273452759)])]\n",
      "\"word 'sent_phishing' not in vocabulary\"\n",
      "[]\n",
      "[('phishing', [('phishing_attacks', 0.8331868052482605), ('phishing_scams', 0.8290295600891113), ('Phishing', 0.8181643486022949), ('phishers', 0.7731481790542603), ('phish', 0.7567018270492554), ('phishing_emails', 0.7406015396118164), ('phishing_e_mails', 0.7268457412719727), ('malware', 0.7232787609100342), ('phishing_schemes', 0.7223339676856995), ('phishing_scam', 0.7034322023391724)])]\n",
      "[('Russian', [('Ukrainian', 0.8035510778427124), ('Russia', 0.746496319770813), ('Kazakh', 0.7438351511955261), ('Belarusian', 0.740909993648529), ('Belarussian', 0.7366286516189575), ('Moscow', 0.7151476144790649), ('Latvian', 0.7052079439163208), ('Georgian', 0.69941246509552), ('Russians', 0.6957246661186218), ('Ukranian', 0.6939061880111694)])]\n"
     ]
    }
   ],
   "source": [
    "topk = 10\n",
    "replacement_ners = []\n",
    "replacement_verbs = []\n",
    "replacement_verbphrases = []\n",
    "replacement_nouns = []\n",
    "replacement_adjectives = []\n",
    "\n",
    "for (i, j) in to_replace_ners:\n",
    "    similar_ners = model.most_similar([i, j.lower()], [], topk)\n",
    "    replacement_ners.append((i, similar_ners))\n",
    "\n",
    "print(replacement_ners)\n",
    "    \n",
    "for verb in to_replace_verbs:\n",
    "    similar_verbs = model.most_similar(verb, [], topk)\n",
    "    replacement_verbs.append((verb,similar_verbs))\n",
    "\n",
    "print(replacement_verbs)\n",
    "\n",
    "for (verbphrase, nn) in to_replace_verbphrases:\n",
    "    try:\n",
    "        similar_verbphrases = model.most_similar([verbphrase, nn], [], topk)\n",
    "        replacement_verbphrases.append((verbphrase, similar_verbphrases))\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "print(replacement_verbphrases)\n",
    "\n",
    "for (verbphrase, nn) in to_replace_verbphrases:\n",
    "    try:\n",
    "        similar_nouns = model.most_similar(nn, [], topk)\n",
    "        replacement_nouns.append((nn, similar_nouns))\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "print(replacement_nouns)\n",
    "\n",
    "for adjective in to_replace_adjectives: \n",
    "    similar_adjectives = model.most_similar(adjective, [], topk)\n",
    "    replacement_adjectives.append((adjective, similar_adjectives))\n",
    "    \n",
    "print(replacement_adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('phishing_attacks', 0.8331868052482605),\n",
       " ('phishing_scams', 0.8290295600891113),\n",
       " ('Phishing', 0.8181643486022949),\n",
       " ('phishers', 0.7731481790542603),\n",
       " ('phish', 0.7567018270492554),\n",
       " ('phishing_emails', 0.7406015396118164),\n",
       " ('phishing_e_mails', 0.7268457412719727),\n",
       " ('malware', 0.7232787609100342),\n",
       " ('phishing_schemes', 0.7223339676856995),\n",
       " ('phishing_scam', 0.7034322023391724),\n",
       " ('phisher', 0.7024546265602112),\n",
       " ('vishing', 0.6804668307304382),\n",
       " ('Phishing_scams', 0.6792201399803162),\n",
       " ('cybercriminals', 0.6781081557273865),\n",
       " ('keylogging', 0.6731592416763306),\n",
       " ('antiphishing', 0.6637416481971741),\n",
       " ('Phishers', 0.6624651551246643),\n",
       " ('scareware', 0.6609221696853638),\n",
       " ('spyware', 0.6607507467269897),\n",
       " ('Zeus_Trojan', 0.6595767140388489),\n",
       " ('ransomware', 0.6550052165985107),\n",
       " ('cyber_criminals', 0.6540520787239075),\n",
       " ('spear_phishing', 0.653903603553772),\n",
       " ('trojan', 0.6471049785614014),\n",
       " ('spammers', 0.6431359052658081),\n",
       " ('Vishing', 0.6406604647636414),\n",
       " ('crimeware', 0.6381171941757202),\n",
       " ('phished', 0.6361656188964844),\n",
       " ('SQL_injection', 0.634614109992981),\n",
       " ('spam', 0.6338978409767151),\n",
       " ('rogueware', 0.6328455805778503),\n",
       " ('identity_theft', 0.6270906925201416),\n",
       " ('spam_mails', 0.6259950399398804),\n",
       " ('cybercrooks', 0.6257392168045044),\n",
       " ('cybercrime', 0.6245124936103821),\n",
       " ('malicious_code', 0.6236134171485901),\n",
       " ('DNS_cache_poisoning', 0.623356282711029),\n",
       " ('cyber_crooks', 0.6226722598075867),\n",
       " ('clickjacking', 0.6215558052062988),\n",
       " ('Koobface', 0.6178711652755737),\n",
       " ('keyloggers', 0.6169607639312744),\n",
       " ('SQL_injection_attacks', 0.6125221848487854),\n",
       " ('keylogger', 0.6113794445991516),\n",
       " ('cybercriminal', 0.6076428890228271),\n",
       " ('fake_antivirus', 0.6073073148727417),\n",
       " ('trojans', 0.6059561967849731),\n",
       " ('Trusteer', 0.6059024333953857),\n",
       " ('DoS_attacks', 0.6042682528495789),\n",
       " ('botnets', 0.601916491985321),\n",
       " ('Malware', 0.6010199785232544)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['phishing'], [], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<North_Carolina_State_University_College_of_Veterinary_Medicine> --> <wikicat_Veterinary_schools_in_the_United_States>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_Natural_history_museums_in_North_Carolina>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wordnet_museum_103800563>\n",
      "<North_Carolina_State_University_reactor_program> --> <wordnet_reactor_104057846>\n",
      "<North_Carolina_State_University> --> <wikicat_Educational_institutions_established_in_1887>\n",
      "<North_Carolina_State_University> --> <wikicat_Universities_and_colleges_in_North_Carolina>\n",
      "<Centennial_Campus_of_North_Carolina_State_University> --> <wikicat_Neighborhoods_in_Raleigh,_North_Carolina>\n",
      "<North_Carolina_State_University_reactor_program> --> <wikicat_Nuclear_research_reactors>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_Museums_in_Raleigh,_North_Carolina>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_Research_museums_in_the_United_States>\n",
      "<Centennial_Campus_of_North_Carolina_State_University> --> <wikicat_Science_parks_in_the_United_States>\n",
      "<North_Carolina_State_University> --> <wordnet_university_108286569>\n",
      "<North_Carolina_State_University> --> <wikicat_Land-grant_universities_and_colleges>\n",
      "<North_Carolina_State_University> --> <wikicat_Universities_and_colleges_in_the_Research_Triangle>\n",
      "<North_Carolina_State_University> --> <wikicat_Universities_and_colleges_in_Raleigh,_North_Carolina>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_University_museums_in_North_Carolina>\n"
     ]
    }
   ],
   "source": [
    "connection = pymysql.connect(host=mydbhost,\n",
    "                             user=mydbuser,\n",
    "                             passwd=mydbpasswd,\n",
    "                             db=mydbdb)\n",
    "\n",
    "#print (\"connect successful!!\")\n",
    "\n",
    "#text = \"A hacker gained access to the PHI for the covered entity's patients.\"\n",
    " \n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Execute SQL select statement\n",
    "        cursor.execute(\"SELECT instance, class FROM simple_types where instance like '%North_Carolina_State_University%'\")\n",
    "        # Commit your changes if writing\n",
    "        # In this case, we are only reading data\n",
    "        # db.commit()\n",
    "        \n",
    "        # Get the number of rows in the resultset\n",
    "        numrows = cursor.rowcount\n",
    "        \n",
    "        # Get and display one row at a time\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            print(row[0], \"-->\", row[1])\n",
    "            \n",
    "# Close the connection\n",
    "finally:\n",
    "    # Close connection.\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Washington_State_University_Reactor> --> <wikicat_Nuclear_research_reactors>\n",
      "<Oregon_State_University_Radiation_Center> --> <wikicat_Nuclear_research_reactors>\n",
      "<North_Carolina_State_University_reactor_program> --> <wikicat_Nuclear_research_reactors>\n",
      "<Purdue_University_Reactor_Number_One> --> <wikicat_Nuclear_research_reactors>\n",
      "<University_of_Massachusetts_Lowell_Radiation_Laboratory> --> <wikicat_Nuclear_research_reactors>\n"
     ]
    }
   ],
   "source": [
    "connection = pymysql.connect(host=mydbhost,\n",
    "                             user=mydbuser,\n",
    "                             passwd=mydbpasswd,\n",
    "                             db=mydbdb)\n",
    "\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Execute SQL select statement\n",
    "        cursor.execute(\"SELECT instance, class FROM simple_types where class like '%wikicat_Nuclear_research_reactors%' and instance like '%university%'\")\n",
    "        \n",
    "        # Get the number of rows in the resultset\n",
    "        numrows = cursor.rowcount\n",
    "        \n",
    "        # Get and display one row at a time\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            print(row[0], \"-->\", row[1])\n",
    "            \n",
    "# Close the connection\n",
    "finally:\n",
    "    # Close connection.\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(['NCSU', 'China'], ['USA'], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
