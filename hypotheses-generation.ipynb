{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Munindar_P._Singh> --> <wordnet_scientist_110560637>\n",
      "<Munindar_P._Singh> --> <wikicat_Artificial_intelligence_researchers>\n",
      "<Munindar_P._Singh> --> <wikicat_Living_people>\n",
      "<Munindar_P._Singh> --> <wikicat_Computer_scientists>\n"
     ]
    }
   ],
   "source": [
    "#Importing configuration\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.txt\")\n",
    "\n",
    "word2vec_path = config.get(\"configuration\",\"word2vec_path\")\n",
    "stanford_corenlp_path = config.get(\"configuration\",\"stanford_corenlp_path\")\n",
    "\n",
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "# Connect\n",
    "mydbhost = config.get(\"configuration\",\"mydbhost\")\n",
    "mydbuser = config.get(\"configuration\",\"mydbuser\")\n",
    "mydbpasswd = config.get(\"configuration\",\"mydbpasswd\")\n",
    "mydbdb = config.get(\"configuration\",\"mydbdb\")\n",
    "connection = pymysql.connect(host=mydbhost,\n",
    "                             user=mydbuser,\n",
    "                             passwd=mydbpasswd,\n",
    "                             db=mydbdb)\n",
    "\n",
    "#print (\"connect successful!!\")\n",
    "\n",
    "text = \"A hacker gained access to the PHI for the covered entity's patients.\"\n",
    " \n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Execute SQL select statement\n",
    "        cursor.execute(\"SELECT instance, class FROM simple_types where instance like '%Munindar%'\")\n",
    "        # Commit your changes if writing\n",
    "        # In this case, we are only reading data\n",
    "        # db.commit()\n",
    "        \n",
    "        # Get the number of rows in the resultset\n",
    "        numrows = cursor.rowcount\n",
    "        \n",
    "        # Get and display one row at a time\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            print(row[0], \"-->\", row[1])\n",
    "            \n",
    "# Close the connection\n",
    "finally:\n",
    "    # Close connection.\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ISIS members threw stones in Paris.\n",
      "Tokenize: ['ISIS', 'members', 'threw', 'stones', 'in', 'Paris', '.']\n",
      "Part of Speech: [('ISIS', 'NNP'), ('members', 'NNS'), ('threw', 'VBD'), ('stones', 'NNS'), ('in', 'IN'), ('Paris', 'NNP'), ('.', '.')]\n",
      "Named Entities: [('ISIS', 'ORGANIZATION'), ('members', 'O'), ('threw', 'O'), ('stones', 'O'), ('in', 'O'), ('Paris', 'CITY'), ('.', 'O')]\n",
      "Constituency Parsing: (ROOT\n",
      "  (S\n",
      "    (NP (NNP ISIS) (NNS members))\n",
      "    (VP (VBD threw)\n",
      "      (NP (NNS stones))\n",
      "      (PP (IN in)\n",
      "        (NP (NNP Paris))))\n",
      "    (. .)))\n",
      "Dependency Parsing: [('ROOT', 0, 3), ('compound', 2, 1), ('nsubj', 3, 2), ('dobj', 3, 4), ('case', 6, 5), ('nmod', 3, 6), ('punct', 3, 7)]\n"
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP(stanford_corenlp_path)\n",
    "connection = pymysql.connect(host=mydbhost,\n",
    "                             user=mydbuser,\n",
    "                             passwd=mydbpasswd,\n",
    "                             db=mydbdb)\n",
    "\n",
    "try: \n",
    "    with connection.cursor() as cursor:\n",
    "        #cursor.execute(\"Select text, day, place, attacker, target, instrument from cyberattacks\")\n",
    "        cursor.execute(\"Select 'ISIS members threw stones in Paris.'\")\n",
    "        numrows = cursor.rowcount\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            sentence = row[0]\n",
    "\n",
    "            print('Sentence:', sentence)\n",
    "            print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "            print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "            print('Named Entities:', nlp.ner(sentence))\n",
    "            print('Constituency Parsing:', nlp.parse(sentence))\n",
    "            print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "finally: \n",
    "    connection.close()\n",
    "    nlp.close() # Do not forget to close! The backend server will consume a lot memery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIS ORGANIZATION\n",
      "Paris CITY\n",
      "threw VBD\n",
      "threw_stones\n"
     ]
    }
   ],
   "source": [
    "# importing StandfordCoreNLP to tokenize, tag, and ner\n",
    "nlp = StanfordCoreNLP(stanford_corenlp_path)\n",
    "\n",
    "sentence = \"ISIS members threw stones in Paris.\"\n",
    "sentence_tokens = nlp.word_tokenize(sentence)\n",
    "sentence_tags = nlp.pos_tag(sentence)\n",
    "sentence_ner = nlp.ner(sentence)\n",
    "#sentence_parse = nlp.parse(sentence)\n",
    "#sentence_dependency = nlp.dependency_parse(sentence)\n",
    "\n",
    "to_replace_ners = []\n",
    "to_replace_verbs = []\n",
    "\n",
    "for (i, j) in sentence_ner:\n",
    "    #print(i, j)\n",
    "    if(j!='O'):\n",
    "        print(i, j)\n",
    "        to_replace_ners.append((i,j))\n",
    "        \n",
    "verb_check = 0\n",
    "        \n",
    "for (i, j) in sentence_tags:\n",
    "    if(verb_check == 1):\n",
    "        verb = verb + '_' + i\n",
    "        to_replace_verbs.append((verb, i))\n",
    "        verb_check = 0\n",
    "        print(verb)\n",
    "    \n",
    "    if(j=='VBD'):\n",
    "        print(i, j)\n",
    "        verb_check = 1\n",
    "        verb = i\n",
    "    \n",
    "        \n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing word2vec to find similarity and neighboring words\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ISIS', [('Isis_Pharmaceuticals_NASDAQ', 0.5358301997184753), ('stratospheric_airship', 0.4330335855484009), ('SILEX', 0.42576032876968384), ('DECIPHER', 0.4255940914154053), ('Isis_Pharmaceuticals_Inc', 0.42540621757507324), ('Corey_Hinderstein', 0.4233326017856598), ('3D_QSAR', 0.417447030544281), ('STFC_Daresbury_Laboratory', 0.41502732038497925), ('Agilent_SureSelect', 0.41367751359939575), ('Excaliard', 0.4103640615940094)]), ('Paris', [('Parisian', 0.6789354085922241), ('Hopital_Europeen_Georges_Pompidou', 0.6536555290222168), ('Spyker_D##_Peking', 0.6336591839790344), ('France', 0.6334909796714783), ('Pantheon_Sorbonne', 0.6312518119812012), ('Aeroports_De', 0.621803879737854), ('Grigny_south', 0.6194689273834229), ('Place_Denfert_Rochereau', 0.6028153896331787), ('guest_Olivier_Dolige', 0.6024351119995117), ('Lazard_Freres_Banque', 0.5998712778091431)])]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "topk = 10\n",
    "replacement_ners = []\n",
    "replacement_verbs = []\n",
    "\n",
    "for (i, j) in to_replace_ners:\n",
    "    similar_ners = model.most_similar(i, [], topk)\n",
    "    replacement_ners.append((i, similar_ners))\n",
    "\n",
    "print(replacement_ners)\n",
    "    \n",
    "for (verb, nn) in replacement_verbs:\n",
    "    similar_verbs = model.most_similar(verb, [], topk)\n",
    "    replacement_verbs.append((verb,similar_verbs))\n",
    "\n",
    "print(replacement_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fudan', 0.5536324381828308),\n",
       " ('Xu', 0.5300264358520508),\n",
       " ('Tsinghua', 0.5270693302154541),\n",
       " ('Zhu', 0.5268391370773315),\n",
       " ('Zhang', 0.5170268416404724),\n",
       " ('Xie', 0.5124308466911316),\n",
       " ('Peking_University', 0.5123456716537476),\n",
       " ('SJTU', 0.5045580863952637),\n",
       " ('Jiao_Tong', 0.501052975654602),\n",
       " ('Feng', 0.4988190531730652),\n",
       " ('Lu', 0.49630722403526306),\n",
       " ('Qiu', 0.4937359094619751),\n",
       " ('Guo', 0.493697851896286),\n",
       " ('Tian', 0.4930003881454468),\n",
       " ('Donghua_University', 0.4914301037788391),\n",
       " ('Tongji', 0.4912794828414917),\n",
       " ('Li', 0.48988276720046997),\n",
       " ('Tsinghua_University', 0.48851269483566284),\n",
       " ('Shangguan', 0.48774391412734985),\n",
       " ('Xiao', 0.48583245277404785),\n",
       " ('Weiming', 0.4857281744480133),\n",
       " ('Jiao_Tong_University', 0.48433077335357666),\n",
       " ('prestigious_Fudan', 0.4840565323829651),\n",
       " ('Ouyang', 0.48237305879592896),\n",
       " ('Nankai_University', 0.47940143942832947),\n",
       " ('Xin', 0.4777411222457886),\n",
       " ('Shenzhen', 0.47725600004196167),\n",
       " ('PolyU', 0.476238876581192),\n",
       " ('Nottingham_Ningbo', 0.475622296333313),\n",
       " ('Huadong', 0.47523754835128784),\n",
       " ('Niu', 0.47383958101272583),\n",
       " ('Xinsheng', 0.4733163118362427),\n",
       " ('Hongyan', 0.472878098487854),\n",
       " ('Fudan_University', 0.4714968800544739),\n",
       " ('National_Chung_Hsing', 0.4711299538612366),\n",
       " ('Zhong', 0.47101473808288574),\n",
       " ('Donghua', 0.4690021872520447),\n",
       " ('HKU', 0.4685346484184265),\n",
       " ('Shuguang', 0.46824151277542114),\n",
       " ('Ye_Qing', 0.4676375985145569),\n",
       " ('Xinghua', 0.4663125276565552),\n",
       " ('Nanjing', 0.46484506130218506),\n",
       " ('Duan', 0.46386852860450745),\n",
       " ('Tianjing', 0.46359318494796753),\n",
       " ('Guang', 0.46341097354888916),\n",
       " ('Chunhui', 0.46298131346702576),\n",
       " ('prestigious_Tsinghua', 0.462302565574646),\n",
       " ('Zheng', 0.4618808329105377),\n",
       " ('Ming_Chuan', 0.4618089199066162),\n",
       " ('Gu', 0.46133267879486084),\n",
       " ('NCKU', 0.4605902135372162),\n",
       " ('Guangming', 0.45977938175201416),\n",
       " ('Hongxing', 0.45949554443359375),\n",
       " ('Jia', 0.45911937952041626),\n",
       " ('Qinghua_University', 0.4588865637779236),\n",
       " ('Tsing_Hua_University', 0.4587530791759491),\n",
       " ('Huo', 0.45817428827285767),\n",
       " ('Gao', 0.4579237699508667),\n",
       " ('Yichen', 0.4576164484024048),\n",
       " ('Weitao', 0.4566299319267273),\n",
       " ('Xiaoming', 0.4563713073730469),\n",
       " ('Xu_Xin', 0.45571619272232056),\n",
       " ('Tianyi', 0.4551964998245239),\n",
       " ('Zhou', 0.4541623592376709),\n",
       " ('Yue', 0.4537648558616638),\n",
       " ('scientist_Karl_Umiker', 0.45364969968795776),\n",
       " ('Linfeng', 0.45355263352394104),\n",
       " ('Purdue', 0.4530203640460968),\n",
       " ('Xuhui_District', 0.4523152709007263),\n",
       " ('Liang', 0.4520817995071411),\n",
       " ('Jiawei', 0.4516534209251404),\n",
       " ('Zheng_Dong', 0.45131707191467285),\n",
       " (\"Xi'an_Jiaotong_University\", 0.4510031044483185),\n",
       " ('Yingbin', 0.4509660303592682),\n",
       " ('Chunhua', 0.45087289810180664),\n",
       " ('Lisheng', 0.4505348801612854),\n",
       " ('Renmin_University', 0.4504636526107788),\n",
       " ('Huazhong_Agricultural_University', 0.45025748014450073),\n",
       " ('Huazhong_University', 0.45015352964401245),\n",
       " ('Gu_Jun', 0.4499365985393524),\n",
       " ('Qi', 0.4496036767959595),\n",
       " ('Yanhua', 0.44905099272727966),\n",
       " ('Liqin', 0.44879865646362305),\n",
       " ('Wenbo', 0.4485260248184204),\n",
       " ('Jianguo', 0.4484596848487854),\n",
       " ('Donglin', 0.4481348991394043),\n",
       " ('Guoyu', 0.4479321539402008),\n",
       " ('Haibin', 0.44792479276657104),\n",
       " ('Huaxing', 0.4477783441543579),\n",
       " ('Zhongping', 0.4476696252822876),\n",
       " ('Guangzhou', 0.4474889636039734),\n",
       " ('Haitao', 0.4473867118358612),\n",
       " ('Zhihao', 0.4469190835952759),\n",
       " ('Zhen', 0.44623756408691406),\n",
       " ('Chen_Jin', 0.4461238384246826),\n",
       " ('Li_Qing', 0.4456042945384979),\n",
       " ('Wenjing', 0.4451407194137573),\n",
       " ('Yihong', 0.4448547959327698),\n",
       " ('Jiao', 0.4445691704750061),\n",
       " ('Lifeng', 0.4443820118904114)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['Cornell', 'Shanghai'], ['New_York'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
