{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing configuration\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.txt\")\n",
    "\n",
    "word2vec_path = config.get(\"configuration\",\"word2vec_path\")\n",
    "stanford_corenlp_path = config.get(\"configuration\",\"stanford_corenlp_path\")\n",
    "\n",
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "# Connect\n",
    "mydbhost = config.get(\"configuration\",\"mydbhost\")\n",
    "mydbuser = config.get(\"configuration\",\"mydbuser\")\n",
    "mydbpasswd = config.get(\"configuration\",\"mydbpasswd\")\n",
    "mydbdb = config.get(\"configuration\",\"mydbdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "# Tree syntax of natural language: http://www.cs.cornell.edu/courses/cs474/2004fa/lec1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing word2vec to find similarity and neighboring words\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=500000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Russian hackers sent phishing emails to compromise NCSU's backup servers.\n",
      "Part of Speech: [('Russian', 'JJ'), ('hackers', 'NNS'), ('sent', 'VBN'), ('phishing', 'VBG'), ('emails', 'NNS'), ('to', 'TO'), ('compromise', 'VB'), ('NCSU', 'NNP'), (\"'s\", 'POS'), ('backup', 'NN'), ('servers', 'NNS'), ('.', '.')]\n",
      "Named Entities: [('Russian', 'NATIONALITY'), ('hackers', 'O'), ('sent', 'O'), ('phishing', 'O'), ('emails', 'O'), ('to', 'O'), ('compromise', 'O'), ('NCSU', 'ORGANIZATION'), (\"'s\", 'O'), ('backup', 'O'), ('servers', 'O'), ('.', 'O')]\n",
      "[('Russian', 'NATIONALITY'), ('NCSU', 'ORGANIZATION')]\n",
      "['sent']\n",
      "[('sent_phishing', 'phishing')]\n",
      "['Russian']\n"
     ]
    }
   ],
   "source": [
    "# importing StandfordCoreNLP to tokenize, tag, and ner\n",
    "nlp = StanfordCoreNLP(stanford_corenlp_path)\n",
    "\n",
    "#sentence = \"The hacker group Anonymous installed ransomware to steal cryptocurrency.\"\n",
    "#sentence = \"ISIL members launched DDOS attack on European Union.\"\n",
    "sentence = \"Russian hackers sent phishing emails to compromise NCSU's backup servers.\"\n",
    "\n",
    "sentence_tokens = nlp.word_tokenize(sentence)\n",
    "sentence_tags = nlp.pos_tag(sentence)\n",
    "sentence_ner = nlp.ner(sentence)\n",
    "#sentence_parse = nlp.parse(sentence)\n",
    "#sentence_dependency = nlp.dependency_parse(sentence)\n",
    "\n",
    "print('Sentence:', sentence)\n",
    "# print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "#print('Constituency Parsing:', nlp.parse(sentence))\n",
    "#print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "\n",
    "to_replace_ners = []\n",
    "to_replace_verbs = []\n",
    "to_replace_verbphrases = []\n",
    "to_replace_adjectives = []\n",
    "to_replace_adjphrase = []\n",
    "# prev_ner = \"\"\n",
    "# current_entity = \"\"\n",
    "# prev_entity_exists = 0\n",
    "\n",
    "for (i, j) in sentence_ner:\n",
    "#     print(i, j)\n",
    "    if(j!='O'):\n",
    "        to_replace_ners.append((i, j))\n",
    "#         print('PREV NER', prev_ner, 'Current NER', j)\n",
    "#         if(j==prev_ner): \n",
    "#             current_entity = current_entity + '_' + i\n",
    "#             print(current_entity)\n",
    "#             prev_ner = j\n",
    "#             prev_entity_exists = 1\n",
    "#         elif(prev_entity_exists): \n",
    "#             to_replace_ners.append((current_entity, prev_ner))\n",
    "#             prev_entity_exists = 0\n",
    "#             current_entity = i\n",
    "#             prev_ner = j\n",
    "#     elif(current_entity != \"\"):     \n",
    "#         to_replace_ners.append((current_entity, prev_ner))\n",
    "#         prev_ner = \"\"\n",
    "#         current_entity = \"\"\n",
    "    \n",
    "        \n",
    "verb_check = 0\n",
    "adj_check = 0\n",
    "        \n",
    "for (i, j) in sentence_tags:\n",
    "    if(verb_check == 1):\n",
    "        verbphrase = verb + '_' + i\n",
    "        to_replace_verbphrases.append((verbphrase, i))\n",
    "        verb_check = 0\n",
    "        #print(verbphrase)\n",
    "    \n",
    "    if(j=='VBD' or j=='VBZ' or j == 'VBP' or j == 'VBN'):\n",
    "        #print(i, j)\n",
    "        to_replace_verbs.append(i)\n",
    "        verb_check = 1\n",
    "        verb = i\n",
    "    \n",
    "    if(j == 'JJ'):\n",
    "        to_replace_adjectives.append(i)\n",
    "        adj = j\n",
    "        adj_check = 1\n",
    "\n",
    "# to_replace_entities = []\n",
    "# prev_type = \"\"\n",
    "# prev_entity = \"\"\n",
    "# cur_entity = \"\"\n",
    "# lastindex = -1\n",
    "# for (entity, type) in to_replace_ners:\n",
    "#     print(entity, type, prev_entity, prev_type, cur_entity)\n",
    "    \n",
    "#     if(type==prev_type):\n",
    "#         to_replace_entities[lastindex] = (to_replace_ners[lastindex][0] + '_' + entity, type)\n",
    "#     else:\n",
    "#         to_replace_entities.append((cur_entity, prev_type))\n",
    "#         cur_entity = entity\n",
    "#         lastindex = lastindex + 1\n",
    "    \n",
    "#     prev_type = type\n",
    "#     #prev_entity = entity\n",
    "\n",
    "print(to_replace_ners)\n",
    "# print(to_replace_entities)\n",
    "print(to_replace_verbs)\n",
    "print(to_replace_verbphrases)\n",
    "print(to_replace_adjectives)\n",
    "        \n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Russian', [('Ukrainian', 0.6549570560455322), ('Kazakh', 0.6192217469215393), ('Latvian', 0.613041341304779), ('Bulgarian', 0.6038623452186584), ('Belarussian', 0.6027601957321167), ('Russians', 0.6012547016143799), ('Ukranian', 0.5897480249404907), ('Romanian', 0.5878135561943054), ('Uzbek', 0.5867130160331726), ('Belarusian', 0.5811043977737427)]), ('NCSU', [('UNC', 0.5804125070571899), ('UNC_CH', 0.5564162135124207), ('UNC_Charlotte', 0.5349904894828796), ('UNC_Chapel_Hill', 0.5265216827392578), ('UNCC', 0.5255768299102783), ('NCCU', 0.511374294757843), ('IPTAY', 0.505825400352478), ('NC', 0.49392879009246826), ('U.Va.', 0.49198469519615173), ('WSSU', 0.4856509566307068)])]\n",
      "[('sent', [('sending', 0.7625813484191895), ('send', 0.736850917339325), ('sends', 0.645559549331665), ('forwarded', 0.618112325668335), ('mailed', 0.5840898752212524), ('despatched', 0.5625948309898376), ('Sending', 0.5511707663536072), ('dispatched', 0.5449301600456238), ('e_mailed', 0.5339891910552979), ('circulated', 0.5030714273452759)])]\n",
      "\"word 'sent_phishing' not in vocabulary\"\n",
      "[]\n",
      "[('phishing', [('phishing_attacks', 0.8331868052482605), ('phishing_scams', 0.8290295600891113), ('Phishing', 0.8181643486022949), ('phishers', 0.7731481790542603), ('phish', 0.7567018270492554), ('phishing_emails', 0.7406015396118164), ('phishing_e_mails', 0.7268457412719727), ('malware', 0.7232787609100342), ('phishing_schemes', 0.7223339676856995), ('phishing_scam', 0.7034322023391724)])]\n",
      "[('Russian', [('Ukrainian', 0.8035510778427124), ('Russia', 0.746496319770813), ('Kazakh', 0.7438351511955261), ('Belarusian', 0.740909993648529), ('Belarussian', 0.7366286516189575), ('Moscow', 0.7151476144790649), ('Latvian', 0.7052079439163208), ('Georgian', 0.69941246509552), ('Russians', 0.6957246661186218), ('Ukranian', 0.6939061880111694)])]\n"
     ]
    }
   ],
   "source": [
    "topk = 10\n",
    "replacement_ners = []\n",
    "replacement_verbs = []\n",
    "replacement_verbphrases = []\n",
    "replacement_nouns = []\n",
    "replacement_adjectives = []\n",
    "\n",
    "for (i, j) in to_replace_ners:\n",
    "    similar_ners = model.most_similar([i, j.lower()], [], topk)\n",
    "    replacement_ners.append((i, similar_ners))\n",
    "\n",
    "print(replacement_ners)\n",
    "    \n",
    "for verb in to_replace_verbs:\n",
    "    similar_verbs = model.most_similar(verb, [], topk)\n",
    "    replacement_verbs.append((verb,similar_verbs))\n",
    "\n",
    "print(replacement_verbs)\n",
    "\n",
    "for (verbphrase, nn) in to_replace_verbphrases:\n",
    "    try:\n",
    "        similar_verbphrases = model.most_similar([verbphrase, nn], [], topk)\n",
    "        replacement_verbphrases.append((verbphrase, similar_verbphrases))\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "print(replacement_verbphrases)\n",
    "\n",
    "for (verbphrase, nn) in to_replace_verbphrases:\n",
    "    try:\n",
    "        similar_nouns = model.most_similar(nn, [], topk)\n",
    "        replacement_nouns.append((nn, similar_nouns))\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "print(replacement_nouns)\n",
    "\n",
    "for adjective in to_replace_adjectives: \n",
    "    similar_adjectives = model.most_similar(adjective, [], topk)\n",
    "    replacement_adjectives.append((adjective, similar_adjectives))\n",
    "    \n",
    "print(replacement_adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('phishing_attacks', 0.8331868052482605),\n",
       " ('phishing_scams', 0.8290295600891113),\n",
       " ('Phishing', 0.8181643486022949),\n",
       " ('phishers', 0.7731481790542603),\n",
       " ('phish', 0.7567018270492554),\n",
       " ('phishing_emails', 0.7406015396118164),\n",
       " ('phishing_e_mails', 0.7268457412719727),\n",
       " ('malware', 0.7232787609100342),\n",
       " ('phishing_schemes', 0.7223339676856995),\n",
       " ('phishing_scam', 0.7034322023391724),\n",
       " ('phisher', 0.7024546265602112),\n",
       " ('vishing', 0.6804668307304382),\n",
       " ('Phishing_scams', 0.6792201399803162),\n",
       " ('cybercriminals', 0.6781081557273865),\n",
       " ('keylogging', 0.6731592416763306),\n",
       " ('antiphishing', 0.6637416481971741),\n",
       " ('Phishers', 0.6624651551246643),\n",
       " ('scareware', 0.6609221696853638),\n",
       " ('spyware', 0.6607507467269897),\n",
       " ('Zeus_Trojan', 0.6595767140388489),\n",
       " ('ransomware', 0.6550052165985107),\n",
       " ('cyber_criminals', 0.6540520787239075),\n",
       " ('spear_phishing', 0.653903603553772),\n",
       " ('trojan', 0.6471049785614014),\n",
       " ('spammers', 0.6431359052658081),\n",
       " ('Vishing', 0.6406604647636414),\n",
       " ('crimeware', 0.6381171941757202),\n",
       " ('phished', 0.6361656188964844),\n",
       " ('SQL_injection', 0.634614109992981),\n",
       " ('spam', 0.6338978409767151),\n",
       " ('rogueware', 0.6328455805778503),\n",
       " ('identity_theft', 0.6270906925201416),\n",
       " ('spam_mails', 0.6259950399398804),\n",
       " ('cybercrooks', 0.6257392168045044),\n",
       " ('cybercrime', 0.6245124936103821),\n",
       " ('malicious_code', 0.6236134171485901),\n",
       " ('DNS_cache_poisoning', 0.623356282711029),\n",
       " ('cyber_crooks', 0.6226722598075867),\n",
       " ('clickjacking', 0.6215558052062988),\n",
       " ('Koobface', 0.6178711652755737),\n",
       " ('keyloggers', 0.6169607639312744),\n",
       " ('SQL_injection_attacks', 0.6125221848487854),\n",
       " ('keylogger', 0.6113794445991516),\n",
       " ('cybercriminal', 0.6076428890228271),\n",
       " ('fake_antivirus', 0.6073073148727417),\n",
       " ('trojans', 0.6059561967849731),\n",
       " ('Trusteer', 0.6059024333953857),\n",
       " ('DoS_attacks', 0.6042682528495789),\n",
       " ('botnets', 0.601916491985321),\n",
       " ('Malware', 0.6010199785232544)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing more attacks like 'phishing' based on W2V\n",
    "\n",
    "model.most_similar(['phishing'], [], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<North_Carolina_State_University_College_of_Veterinary_Medicine> --> <wikicat_Veterinary_schools_in_the_United_States>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_Natural_history_museums_in_North_Carolina>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wordnet_museum_103800563>\n",
      "<North_Carolina_State_University_reactor_program> --> <wordnet_reactor_104057846>\n",
      "<North_Carolina_State_University> --> <wikicat_Educational_institutions_established_in_1887>\n",
      "<North_Carolina_State_University> --> <wikicat_Universities_and_colleges_in_North_Carolina>\n",
      "<Centennial_Campus_of_North_Carolina_State_University> --> <wikicat_Neighborhoods_in_Raleigh,_North_Carolina>\n",
      "<North_Carolina_State_University_reactor_program> --> <wikicat_Nuclear_research_reactors>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_Museums_in_Raleigh,_North_Carolina>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_Research_museums_in_the_United_States>\n",
      "<Centennial_Campus_of_North_Carolina_State_University> --> <wikicat_Science_parks_in_the_United_States>\n",
      "<North_Carolina_State_University> --> <wordnet_university_108286569>\n",
      "<North_Carolina_State_University> --> <wikicat_Land-grant_universities_and_colleges>\n",
      "<North_Carolina_State_University> --> <wikicat_Universities_and_colleges_in_the_Research_Triangle>\n",
      "<North_Carolina_State_University> --> <wikicat_Universities_and_colleges_in_Raleigh,_North_Carolina>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_University_museums_in_North_Carolina>\n"
     ]
    }
   ],
   "source": [
    "# Get categories for NCSU from Wikipedia\n",
    "# Need to hardcode NCSU as North_Carolina_State_University as there is no instance as NCSU in Wikipedia ontology\n",
    "\n",
    "connection = pymysql.connect(host=mydbhost,\n",
    "                             user=mydbuser,\n",
    "                             passwd=mydbpasswd,\n",
    "                             db=mydbdb)\n",
    "\n",
    "replacement_institutes = []\n",
    "\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Execute SQL select statement\n",
    "        cursor.execute(\"SELECT instance, class FROM simple_types where instance like '%North_Carolina_State_University%'\")\n",
    "        # Commit your changes if writing\n",
    "        # In this case, we are only reading data\n",
    "        # db.commit()\n",
    "        \n",
    "        # Get the number of rows in the resultset\n",
    "        numrows = cursor.rowcount\n",
    "        \n",
    "        # Get and display one row at a time\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            print(row[0], \"-->\", row[1])\n",
    "            replacement_institutes.append(('NCSU', row[1]))\n",
    "# Close the connection\n",
    "finally:\n",
    "    # Close connection.\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing category based hypotheses\n",
      "[\"Russian hackers sent phishing emails to compromise <wikicat_Veterinary_schools_in_the_United_States>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Natural_history_museums_in_North_Carolina>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wordnet_museum_103800563>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wordnet_reactor_104057846>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Educational_institutions_established_in_1887>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Universities_and_colleges_in_North_Carolina>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Neighborhoods_in_Raleigh,_North_Carolina>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Nuclear_research_reactors>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Museums_in_Raleigh,_North_Carolina>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Research_museums_in_the_United_States>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Science_parks_in_the_United_States>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wordnet_university_108286569>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Land-grant_universities_and_colleges>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Universities_and_colleges_in_the_Research_Triangle>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_Universities_and_colleges_in_Raleigh,_North_Carolina>'s backup servers.\", \"Russian hackers sent phishing emails to compromise <wikicat_University_museums_in_North_Carolina>'s backup servers.\"]\n"
     ]
    }
   ],
   "source": [
    "# Listing category based hypotheses \n",
    "\n",
    "category_hypotheses = []\n",
    "\n",
    "for (institute, category) in replacement_institutes: \n",
    "    category_hypotheses.append(sentence.replace(institute, category))\n",
    "    \n",
    "print(\"Listing category based hypotheses\")\n",
    "print(category_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Washington_State_University_Reactor> --> <wikicat_Nuclear_research_reactors>\n",
      "<Oregon_State_University_Radiation_Center> --> <wikicat_Nuclear_research_reactors>\n",
      "<North_Carolina_State_University_reactor_program> --> <wikicat_Nuclear_research_reactors>\n",
      "<Purdue_University_Reactor_Number_One> --> <wikicat_Nuclear_research_reactors>\n",
      "<University_of_Massachusetts_Lowell_Radiation_Laboratory> --> <wikicat_Nuclear_research_reactors>\n"
     ]
    }
   ],
   "source": [
    "# Universities with nuclear research reactor\n",
    "connection = pymysql.connect(host=mydbhost,\n",
    "                             user=mydbuser,\n",
    "                             passwd=mydbpasswd,\n",
    "                             db=mydbdb)\n",
    "\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Execute SQL select statement\n",
    "        cursor.execute(\"SELECT instance, class FROM simple_types where class like '%wikicat_Nuclear_research_reactors%' and instance like '%university%'\")\n",
    "        \n",
    "        # Get the number of rows in the resultset\n",
    "        numrows = cursor.rowcount\n",
    "        \n",
    "        # Get and display one row at a time\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            print(row[0], \"-->\", row[1])\n",
    "            \n",
    "# Close the connection\n",
    "finally:\n",
    "    # Close connection.\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Weatherspoon_Art_Museum> --> <wikicat_University_museums_in_North_Carolina>\n",
      "<Ackland_Art_Museum> --> <wikicat_University_museums_in_North_Carolina>\n",
      "<Nasher_Museum_of_Art> --> <wikicat_University_museums_in_North_Carolina>\n",
      "<North_Carolina_State_University_Insect_Museum> --> <wikicat_University_museums_in_North_Carolina>\n"
     ]
    }
   ],
   "source": [
    "# Universities museums in NC\n",
    "connection = pymysql.connect(host=mydbhost,\n",
    "                             user=mydbuser,\n",
    "                             passwd=mydbpasswd,\n",
    "                             db=mydbdb)\n",
    "\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Execute SQL select statement\n",
    "        cursor.execute(\"SELECT instance, class FROM simple_types where class like '%wikicat_University_museums_in_North_Carolina%'\")\n",
    "        \n",
    "        # Get the number of rows in the resultset\n",
    "        numrows = cursor.rowcount\n",
    "        \n",
    "        # Get and display one row at a time\n",
    "        for x in range(0, numrows):\n",
    "            row = cursor.fetchone()\n",
    "            print(row[0], \"-->\", row[1])\n",
    "            \n",
    "# Close the connection\n",
    "finally:\n",
    "    # Close connection.\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UNC_CH', 0.506248414516449),\n",
       " ('UNC_Chapel_Hill', 0.5013554692268372),\n",
       " ('Tsinghua', 0.4970877766609192),\n",
       " ('Anhui', 0.48243093490600586),\n",
       " ('Liaoning', 0.48201996088027954),\n",
       " ('Fudan', 0.4745294451713562),\n",
       " ('Zhejiang', 0.4721640944480896),\n",
       " ('yuan', 0.4709556996822357),\n",
       " ('NCCU', 0.47059300541877747),\n",
       " ('Tsinghua_University', 0.4681386947631836),\n",
       " ('Guangdong', 0.4578723609447479),\n",
       " ('Guizhou', 0.45680493116378784),\n",
       " ('Hubei', 0.4539087116718292),\n",
       " ('Yuan', 0.45208263397216797),\n",
       " ('Anhui_province', 0.45119917392730713),\n",
       " ('Yunnan', 0.4510120153427124),\n",
       " ('Shaanxi', 0.4507513642311096),\n",
       " ('China_Hunan_Province', 0.4505188465118408),\n",
       " ('UNC_Charlotte', 0.4503973424434662),\n",
       " ('trillion_yuan', 0.4488682150840759),\n",
       " ('Sichuan', 0.4478117525577545),\n",
       " ('Peking_University', 0.4472169578075409),\n",
       " ('Liaoning_province', 0.4455345571041107),\n",
       " ('Guangxi_Zhuang_Autonomous_Region', 0.4436618685722351),\n",
       " ('Yunnan_province', 0.4407528042793274),\n",
       " ('Jilin_Province', 0.4382138252258301),\n",
       " ('Chinese', 0.43539175391197205),\n",
       " ('Hunan_Province', 0.4332261085510254),\n",
       " ('Guangxi', 0.4329456686973572),\n",
       " ('Henan_province', 0.4299122393131256),\n",
       " ('Hunan_province', 0.42937085032463074),\n",
       " ('Qinghua_University', 0.42904266715049744),\n",
       " ('Chongqing', 0.4277913570404053),\n",
       " ('Tongji', 0.427508145570755),\n",
       " ('Beijing', 0.4266324043273926),\n",
       " ('Hainan', 0.4255988597869873),\n",
       " ('Shenyang', 0.4252546727657318),\n",
       " ('Jiangxi', 0.42524197697639465),\n",
       " ('Sichuan_province', 0.42521291971206665),\n",
       " ('Jiangsu_Province', 0.4248543381690979),\n",
       " ('Jilin', 0.42483246326446533),\n",
       " ('MIIT', 0.4246714115142822),\n",
       " ('UNC', 0.42459091544151306),\n",
       " ('Guangdong_Province', 0.4244426488876343),\n",
       " ('Shandong_province', 0.42427149415016174),\n",
       " ('NDRC', 0.42135128378868103),\n",
       " ('UNCG', 0.42135071754455566),\n",
       " ('Ningxia', 0.42101165652275085),\n",
       " ('Beijing_Tsinghua', 0.42014262080192566),\n",
       " ('Xugong', 0.41956478357315063)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['NCSU', 'China'], ['USA'], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
